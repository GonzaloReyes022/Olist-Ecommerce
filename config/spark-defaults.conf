# ==========================================
# DELTA LAKE CONFIGURATION
# ==========================================
spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension
spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog

# ¿Qué hacen estas configs?
# - Habilitan Delta Lake en Spark
# - Permiten usar SQL Delta (MERGE, DELETE, UPDATE)

# ==========================================
# DELTA LAKE OPTIMIZATIONS
# ==========================================
spark.databricks.delta.optimizeWrite.enabled=true
# Optimiza escrituras automáticamente

spark.databricks.delta.autoCompact.enabled=true
# Compacta archivos pequeños automáticamente

spark.databricks.delta.properties.defaults.enableChangeDataFeed=true
# Habilita Change Data Capture (CDC)

# ==========================================
# PERFORMANCE
# ==========================================
spark.sql.adaptive.enabled=true
# Adaptive Query Execution - optimiza queries dinámicamente

spark.sql.adaptive.coalescePartitions.enabled=true
# Reduce particiones automáticamente

spark.sql.files.maxPartitionBytes=134217728
# 128MB por partición (se ajusta según el caso)

# ==========================================
# MEMORY
# ==========================================
spark.driver.memory=4g
spark.executor.memory=4g
spark.driver.maxResultSize=2g

# ==========================================
# CHECKPOINTING
# ==========================================
spark.sql.streaming.checkpointLocation=/opt/data/checkpoints
# Para streaming jobs